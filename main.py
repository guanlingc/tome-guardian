import os
from typing import Sequence
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph, add_messages
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage, AIMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from typing_extensions import Annotated, TypedDict
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI

# define the state class
class State(TypedDict):
        messages: Annotated[Sequence[BaseMessage], add_messages]


    ## Functions 
def chat_w_llm(text: str, llm:object, config:dict):
    """
    Sends the input text to a language model and returns the generated response content.

    Args:
        text (str): The input text prompt to send to the language model.
        config (dict): contains session id for the memory portion 

    Returns:
        str: The content of the response generated by the language model.
    """
    input= [HumanMessage(text)]
    response = llm.invoke({"messages": input}, config=config)
    return response['messages'][-1].content


def get_env_variables():
    """
    creates a dictionary with all the env variables within it.
    Loads environment variables from a .env file 
    Raises:
        EnvironmentError: If the "GOOGLE_API_KEY" is not found in the environment variables.
    Returns:
        str: The Google API key.
    """
    # load .env files
    load_dotenv()
    # define google gemini api key
    env = {
        "GOOGLE_API_KEY" : os.getenv("GOOGLE_API_KEY"),
        "GEMINI_MODEL" : os.getenv("GEMINI_MODEL")
    }
    # raise error is api key not present
    if not all(env.values()):
        raise EnvironmentError("Please check your .env file.")
    return env
    
    
def main():
    # load environment variables (ie google gemini key)
    env= get_env_variables()
    # define llm model and chat conditions
    llm = ChatGoogleGenerativeAI(model=env["GEMINI_MODEL"], temperature = 0.3)
    # define system behaviour of LLM, messages placeholder to contain the chat history
    system_prompt = ChatPromptTemplate.from_messages([
        ("system","You are a helpful assistant"),
        MessagesPlaceholder(variable_name="messages")
    ])
    # function for to call LLM    
    def call_model(state: State):
        prompt = system_prompt.invoke(state)
        response = llm.invoke(prompt)
        return {"messages": response}
    # create the workflow graph with a Start Node and connect to a model node that calls LLM
    workflow = StateGraph(state_schema=State)
    workflow.add_edge(START, "model")
    workflow.add_node("model", call_model)
    # creates a memory function for LLM 
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    config = {"configurable": {"thread_id": "123"}}
    # while loop to keep chatting
    while True:
        # get user input 
        user_text = input("User: ")
        if user_text == "exit":
            print("Goodbye!")
            break
        # pass user input to LLM as the prompt
        response = chat_w_llm(user_text, app, config)
        print(f"Gemini: {response}")

# runs script only when it is executed directly
if __name__ == "__main__":
    main()
