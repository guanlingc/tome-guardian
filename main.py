import os
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI

from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

    ## Functions 
def chat_w_llm(text: str, llm:object):
    """
    Sends the input text to a language model and returns the generated response content.

    Args:
        text (str): The input text prompt to send to the language model.

    Returns:
        str: The content of the response generated by the language model.
    """
    response = llm.invoke(text)
    return response["response"]

def get_api_key():
    """
    Retrieves the Google API key from the environment variables.
    Loads environment variables from a .env file and attempts to fetch the value of "GOOGLE_API_KEY".
    Raises:
        EnvironmentError: If the "GOOGLE_API_KEY" is not found in the environment variables.
    Returns:
        str: The Google API key.
    """
    # load .env files
    load_dotenv()
    # define google gemini api key
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
    # raise error is api key not present
    if not GOOGLE_API_KEY:
        raise EnvironmentError("Please place API key in .env file.")
    
    
def main():
    # load environment variables (ie google gemini key)
    get_api_key()
    # define the memory module
    memory = ConversationBufferMemory()
    # define llm model and chat conditions
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature = 0.3)
    # Put them in the conversation chain
    llm_chat = ConversationChain(llm=llm, memory=memory)
    # while loop to keep chatting
    while True:
        user_text = input("User: ")
        if user_text == "exit":
            print("Goodbye!")
            chat=False
            break
        response = chat_w_llm(user_text, llm_chat)
        print(f"Gemini: {response}")

# runs script only when it is executed directly
if __name__ == "__main__":
    main()
