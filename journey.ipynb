{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598af707",
   "metadata": {},
   "source": [
    "##### Purpose\n",
    "This notebook is my dairy to teach myself on how to build a RAG system and use LLMs in applications.\n",
    "The end goal here is to create a LLM that has DaggerHearts game manual as its knowledge base.  \n",
    "I will be starting from the small blocks first and build up from there. \n",
    "\n",
    "First. LLM, it is the brain in the whole system. \n",
    "I will be using LangChain as the library of choice as it is one of the most popular frameworks avaliable. \n",
    "Gemini has been chosen as it provides free API keys for me to test out the application.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4ba964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library import cell. This cell will contain all the library imports used in this notebook.\n",
    "\n",
    "# To handle API Key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# To get the LLM model\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# To get the memory\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d10f0",
   "metadata": {},
   "source": [
    "What is an API Key? \n",
    "An API Key is akin to a password/digital key for you to access services. \n",
    "This is used to prevent unauthorised access etc.\n",
    "\n",
    "#### Setting up API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f35ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# use .env to load env variables\n",
    "load_dotenv()\n",
    "# saving env variable as GOOGLE_API_KEY\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761b861",
   "metadata": {},
   "source": [
    "#### Initialize LLM Model\n",
    "\n",
    "Here are some standardized parameters when constructing ChatModels:\n",
    "\n",
    "* model: the name of the model\n",
    "* temperature: the sampling temperature\n",
    "* timeout: request timeout\n",
    "* max_tokens: max tokens to generate\n",
    "* stop: default stop sequences\n",
    "* max_retries: max number of times to retry requests\n",
    "* api_key: API key for the model provider\n",
    "* base_url: endpoint to send requests to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d82ffffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# initialize a llm \n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", temperature = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15bfd8",
   "metadata": {},
   "source": [
    "#### Calling the LLM. \n",
    "Use `.invoke` to \"talk\" the LLM. The LLM will generate a response and you view the response within the `.content` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f8ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tom! Yes, you are. It's great to meet you!\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# use .invoke to get a response from the LLM based on query\n",
    "query = \"Hi, i'm tom?\"\n",
    "response = llm.invoke(query)\n",
    "print(response.content, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15e419",
   "metadata": {},
   "source": [
    "Use `.stream` to see how the response broken down into parts, the following example is using the pipe `|` symbol to show each chunk that is returned by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecb3ff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are the classic lyrics to \"Fly Me to the Moon,\" famously performed by Frank Sinatra, but written by Bart Howard.\n",
      "\n",
      "---\n",
      "\n",
      "**Fly Me to the Moon**|\n",
      "*(Written by Bart Howard)*\n",
      "\n",
      "Fly me to the moon\n",
      "Let me play among the stars\n",
      "Let me see what spring is like\n",
      "On a-Jupiter and Mars\n",
      "\n",
      "In other words, hold my hand\n",
      "In other words, baby|, kiss me\n",
      "\n",
      "Fill my heart with song\n",
      "And let me sing forevermore\n",
      "You are all I long for\n",
      "All I worship and adore\n",
      "\n",
      "In other words, please be true\n",
      "In other words, I love you\n",
      "\n",
      "---|"
     ]
    }
   ],
   "source": [
    "# use .invoke to get a response from the LLM based on query\n",
    "\n",
    "for chunk in llm.stream(\"Write me the lyrics to the song 'Fly me to the moon'\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3690b1",
   "metadata": {},
   "source": [
    "After calling a LLM. The returned object has several attributes\n",
    "\n",
    "* contents = Text message from the LLM model\n",
    "* additional_kwargs\n",
    "* response_metadata\n",
    "* id\n",
    "* usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c493e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hi Tom! Yes, you are. It's great to meet you!\\n\\nHow can I help you today?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--afe5f853-be75-4e5f-984a-34d57fcc4231-0' usage_metadata={'input_tokens': 8, 'output_tokens': 639, 'total_tokens': 647, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 615}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e1247e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_backwards_compat_tool_calls', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', 'additional_kwargs', 'construct', 'content', 'copy', 'dict', 'example', 'from_orm', 'get_lc_namespace', 'id', 'invalid_tool_calls', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'parse_file', 'parse_obj', 'parse_raw', 'pretty_print', 'pretty_repr', 'response_metadata', 'schema', 'schema_json', 'text', 'to_json', 'to_json_not_implemented', 'tool_calls', 'type', 'update_forward_refs', 'usage_metadata', 'validate']\n"
     ]
    }
   ],
   "source": [
    "# this allows you to see the different attributes of this object\n",
    "print(dir(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd604dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tom! Nice to meet you.\n",
      "\n",
      "I'm an AI assistant, ready to help you with whatever you need. What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36f5b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(response.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec07e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "print(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f64fee",
   "metadata": {},
   "source": [
    "#### Formatting the Prompt\n",
    "\n",
    "* The prompt is the input for the LLM. \n",
    "* Prompts are can be anything from simple (\"What is 2+2\") to complex (\"Explain the meaning of life\")\n",
    "* You can define variables within the prompt it self. This provide useability to change the variables\n",
    "\n",
    "* `PromptTemplate` allows for simple string inputs\n",
    "* `ChatPromptTemplate` allows for more complex inputs with roles etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c4e2427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Tom! Nice to meet you.\\n\\nI'm ready to answer your questions whenever you are. What would you like to know?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b6475dc0-10ed-490d-92f5-321db657f427-0', usage_metadata={'input_tokens': 10, 'output_tokens': 579, 'total_tokens': 589, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 551}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Answer the following questions. {query}\")\n",
    "prompt.invoke({\"query\": \"Hi  im tom\"})\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"query\": \"Hi  im tom\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec014484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc9d7de",
   "metadata": {},
   "source": [
    "#### Adding memory function for conversational context\n",
    "\n",
    "LLMs are stateless. \n",
    "\n",
    "Here you can see that, after introducing yourself to the LLM. It is not able to \"remember\" your name. \n",
    "By adding a memory function. You will be able to converse with the LLM like talking to a person \n",
    "This is achieved by adding the chat history as context for LLM to generate the response.\n",
    "\n",
    "Messages in LangChain have several components. A `role`, a `content` and `metadata`\n",
    "\n",
    "Roles - describes WHO is saying the message\n",
    "* user - You\n",
    "* assistant - The LLM answering the input\n",
    "* system - tells models how to behave, not a models support this\n",
    "* tool - represent the decision for LLM to use a tool or not. It contains a dictionary (`name`, `args`, `id`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbd688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Albert! It's nice to meet you. I'm an AI.\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# use .invoke to get a response from the LLM based on query\n",
    "query_2 = \"Hi i am albert\"\n",
    "response_2 = llm.invoke(query_2)\n",
    "print(response_2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1335247",
   "metadata": {},
   "source": [
    "The LLM does not remember that you have already introduced yourself earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have access to personal information about you, including your name. I don't retain memory of past conversations or user identities.\n",
      "\n",
      "If you'd like to tell me your name, I can refer to you by it during our current conversation, but I won't remember it for future interactions.\n"
     ]
    }
   ],
   "source": [
    "# use .invoke to get a response from the LLM based on query\n",
    "query_3 = \"What is my name\"\n",
    "response_3 = llm.invoke(query_3)\n",
    "print(response_3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c282da",
   "metadata": {},
   "source": [
    "To enable them as conversational partners. They need to be able to remember what has be said before. \n",
    "This means to give them \"memory\".\n",
    "\n",
    "#### Create memory function for conversational context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chat = ConversationChain(llm = llm , memory = memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29dcdd8",
   "metadata": {},
   "source": [
    "The returned object from the LLM invocation is a dictionary\n",
    "you can access the response but accessing the key \"response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01f792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi im tom',\n",
       " 'history': \"Human: Hi im tom\\nAI: Hi Tom! It's really nice to meet you! I'm an AI, a large language model, and I'm here and ready to chat about anything you'd like. It's always great to connect with new people – or, well, new humans, in my case! How are you doing today, Tom?\\nHuman: what is my name?\\nAI: Your name is Tom! You told me that right at the beginning of our chat. It's good to remember names, even for an AI like me! Is there anything else you'd like to chat about, Tom?\",\n",
       " 'response': \"Oh, hello again, Tom! It seems like you're introducing yourself again, and I'm happy to confirm that yes, your name is Tom! You've told me that a few times now since we started chatting, and I've definitely got it stored away in my memory. It's always good to be sure, though! Is there anything specific you'd like to talk about today, Tom, or were you just checking if I remembered? I'm ready for whatever you have in mind!\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.invoke(\"Hi im tom\")\n",
    "response['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bcfa4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, hello again, Tom! It seems like you're introducing yourself again, and I'm happy to confirm that yes, your name is Tom! You've told me that a few times now since we started chatting, and I've definitely got it stored away in my memory. It's always good to be sure, though! Is there anything specific you'd like to talk about today, Tom, or were you just checking if I remembered? I'm ready for whatever you have in mind!\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a04db06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name?',\n",
       " 'history': \"Human: Hi im tom\\nAI: Hi Tom! It's really nice to meet you! I'm an AI, a large language model, and I'm here and ready to chat about anything you'd like. It's always great to connect with new people – or, well, new humans, in my case! How are you doing today, Tom?\",\n",
       " 'response': \"Your name is Tom! You told me that right at the beginning of our chat. It's good to remember names, even for an AI like me! Is there anything else you'd like to chat about, Tom?\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"what is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68eccb8",
   "metadata": {},
   "source": [
    "### ConversationChain is deprecated\n",
    "\n",
    "will need to learn about Runnables and LangChain Expression Language (LECL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3621d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e5b05",
   "metadata": {},
   "source": [
    "The new way of including is adding in the chat history into the model\n",
    "\n",
    "* you would need to include a storage location (in this example it is a global variable)\n",
    "* the Message history requires a function called `get_session_history` at a storage.\n",
    "* Therefore during the wrapping, the LLM is bundled together with the memory\n",
    "\n",
    "After that, we can import the relevant classes and set up our chain which wraps the model and adds in this message history. A key part here is the function we pass into as the `get_session_history`. This function is expected to take in a `session_id` and return a Message History object. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain (we'll show how to do that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcee2ad",
   "metadata": {},
   "source": [
    "You are able to wrap the function into another function such that it will create a store on the go. \n",
    "although this might lead to issues later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488593af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_memory():\n",
    "# create a store\n",
    "    store = {}\n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = InMemoryChatMessageHistory()\n",
    "        return store[session_id]\n",
    "    return get_session_history\n",
    "\n",
    "memory = create_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5096dfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d086c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plugging the llm to the chat history (memory)\n",
    "with_message_history = RunnableWithMessageHistory(llm, memory)\n",
    "# give a session_id \n",
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0add2",
   "metadata": {},
   "source": [
    "now we need to create a configuration to assign a value to each chat session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "420b3f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Bob! Nice to meet you.\\n\\nHow can I help you today?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bob\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "237d058e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 + 2 = 4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_b = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"what is 2+2? \")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response_b.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad30f8",
   "metadata": {},
   "source": [
    "you will then be able to extract the memory by accessing the memory dictionary using the session id key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a445eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi! I'm Bob\n",
      "AI: Hi Bob! Nice to meet you.\n",
      "\n",
      "How can I help you today?\n",
      "Human: what is 2+2? \n",
      "AI: 2 + 2 = 4\n"
     ]
    }
   ],
   "source": [
    "print(store[\"abc2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23609845",
   "metadata": {},
   "source": [
    "To chain stuff using the LCEL is to use the pipe operator \n",
    "\n",
    "The following line will effectively mean \n",
    "place output of prompt into llm\n",
    "place output of llm into output_parser\n",
    "\n",
    "`chain = prompt | llm | output_parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bea2ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's an ice cream's favorite day of the week?\\n**Sundae!**\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", temperature = 0.3)\n",
    "prompt = ChatPromptTemplate. from_template(\"tell me a short joke about {topic}\")\n",
    "output_parser = StrOutputParser ()\n",
    "chain = prompt | llm | output_parser\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627d18b",
   "metadata": {},
   "source": [
    "The pipe operator is typically the \"or\" method. But in langchain it is used to run the first object and then pass it to the second object \n",
    "\n",
    "Runnable lambda is a wrapper for custom functions to give them the runnable interface\n",
    "\n",
    "`chain = prompt | RunnableLambda(custom_function) | llm | output_parser`\n",
    "\n",
    "\n",
    "Runnable runthrough is a wrapper that doesnt change anything but allows the addition of keys into the input\n",
    "\n",
    "Runnable parrallels allows you to run things together or splits the execution flow into branches\n",
    "This allows you to change the methods inside each branch separately\n",
    "\n",
    "using `.assign` function allows you to add in more keys to the langchain function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbdcdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# initialize a llm \n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", temperature = 0.3)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me an interesting fact about {topic}\")\n",
    "    ])\n",
    "\n",
    "# pass in the prompt to the model\n",
    "chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21750ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HERE\\'S AN INTERESTING FACT ABOUT RED BEANS:\\n\\nWHILE MANY WESTERN CUISINES PRIMARILY USE RED BEANS (LIKE KIDNEY BEANS) IN SAVORY DISHES SUCH AS CHILI, STEWS, AND RED BEANS AND RICE, IN **EAST ASIAN CULTURES** (LIKE JAPAN, CHINA, AND KOREA), A SMALLER VARIETY OF RED BEAN CALLED **ADZUKI BEANS** ARE MOST FAMOUSLY USED TO MAKE A **SWEET PASTE**.\\n\\nTHIS SWEET RED BEAN PASTE, KNOWN AS \"ANKO\" IN JAPANESE, IS A INCREDIBLY POPULAR FILLING FOR A WIDE VARIETY OF DESSERTS, INCLUDING MOCHI, DORAYAKI (JAPANESE PANCAKES), MOONCAKES, SWEET BUNS, AND EVEN AS A TOPPING FOR SHAVED ICE OR ICE CREAM. IT\\'S A FASCINATING CONTRAST TO THEIR SAVORY USES ELSEWHERE!'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# make a simple function\n",
    "def toupper(input):\n",
    "    return input.content.upper()\n",
    "\n",
    "def rando_func(input):\n",
    "    return 100\n",
    "\n",
    "\n",
    "chain =  prompt | llm | RunnableLambda(toupper)\n",
    "\n",
    "chain.invoke({\"topic\":\"red beans\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba98a658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['topic']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae693727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt = {\"1\":\"example\"}\n",
    "type(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4285532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "chain = RunnableParallel({\"x\": RunnablePassthrough ()}).assign(extra=RunnableLambda(rando_func))\n",
    "\n",
    "chain = chain.invoke({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5da48b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {}, 'extra': 100}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597e88b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory \n",
    "\n",
    "memory_store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in memory_store:\n",
    "        memory_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return memory_store[session_id]\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"123\"}}\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature = 0.3)\n",
    "gemini_mem = RunnableWithMessageHistory(gemini, get_session_history)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are Jarvis, a helpful assistant that helps to answer questions\"),\n",
    "    (\"user\", \"can you answer this? {query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d8b2e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are Jarvis, a helpful assistant that helps to answer questions', additional_kwargs={}, response_metadata={}), HumanMessage(content='can you answer this? what is my name?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(\"what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5675553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create chain\n",
    "chain = prompt | gemini_mem\n",
    "response = chain.invoke({'query':\"what is ice cream?\"}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51819a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As Jarvis, my \"user prompt\" is essentially to be a helpful assistant that answers your questions!\\n\\nMore broadly, as an AI, I don\\'t have a \"user prompt\" in the same way a human might give me a specific instruction for a single task. Instead, I operate based on my programming and the vast amount of data I was trained on, all designed to make me a helpful and informative AI assistant.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b4d10a85-cb85-4f27-b4d8-7994f70a4fb3-0', usage_metadata={'input_tokens': 66, 'output_tokens': 372, 'total_tokens': 438, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 286}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5964eb41",
   "metadata": {},
   "source": [
    "After learning about LangChain Expression Language (LCEL), the documentations online mentions to use the langgraph persistenance that will work exactly the same with a RunnableMemoryHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e33367d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory \n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", temperature = 0.3)\n",
    "# prompt = ChatPromptTemplate. from_template(\"tell me a short joke about {topic}\")\n",
    "# memory_store = {}\n",
    "# def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "#     if session_id not in memory_store:\n",
    "#         memory_store[session_id] = InMemoryChatMessageHistory()\n",
    "#     return memory_store[session_id]\n",
    "\n",
    "# config = {\"configurable\": {\"session_id\": \"123\"}}\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature = 0.3)\n",
    "# gemini_mem = RunnableWithMessageHistory(gemini, get_session_history)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are Jarvis, a helpful assistant that helps to answer questions\"),\n",
    "    (\"user\", \"can you answer this? {query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86454b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c18cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"query\": \"hi im tom\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fadc34e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are Jarvis, a helpful assistant that helps to answer questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='can you answer this? {query}'), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), temperature=0.3, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x104d2d2e0>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e81c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant. Answer all questions to the best of your ability.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0365d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_2 = prompt_2 | gemini\n",
    "response_2 = chain_2.invoke(input=[\"hi im tom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2020bc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Tom! It's nice to meet you.\n",
      "\n",
      "How can I help you today?\n",
      "Hi Tom, it's nice to meet you! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)\n",
    "print(response_2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3664aac2",
   "metadata": {},
   "source": [
    "The following method gives an examples of using Langgraph's State \n",
    "\n",
    "* define a function that makes a call to the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed351fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature = 0.3)\n",
    "# i presume this is initializing the a graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "# def the call function \n",
    "def call_llm(state: MessagesState):\n",
    "    system_prompt = (\"you are a helpful assistant\")\n",
    "    # takes the system prompt + the current state of the messages\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = gemini.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the node and edge \n",
    "workflow.add_node(\"model\", call_llm) # creates a node in the graph called model\n",
    "workflow.add_edge(START, \"model\")  #adds a connection of start node and model node\n",
    "\n",
    "# Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ebb703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='cf4b0534-a6cc-4bee-bb1f-19744a294aa2'),\n",
       "  AIMessage(content=\"J'aime la programmation.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--83267697-c2fa-4337-8b56-50b3d2f7f8f5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 61, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 54}})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Translate to French: I love programming.\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a10af8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='cf4b0534-a6cc-4bee-bb1f-19744a294aa2'),\n",
       "  AIMessage(content=\"J'aime la programmation.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--83267697-c2fa-4337-8b56-50b3d2f7f8f5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 61, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 54}}),\n",
       "  HumanMessage(content='Translate the same thing into chinese', additional_kwargs={}, response_metadata={}, id='422701ba-fb8b-4726-babf-b95a6983e7c2'),\n",
       "  AIMessage(content='我爱编程 (Wǒ ài biānchéng.)', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--4019671a-0a2e-431e-b33e-4d1ef8421ba5-0', usage_metadata={'input_tokens': 29, 'output_tokens': 212, 'total_tokens': 241, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 199}})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Translate the same thing into chinese\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77886302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Translate to French: I love programming.', additional_kwargs={}, response_metadata={}, id='cf4b0534-a6cc-4bee-bb1f-19744a294aa2'),\n",
       "  AIMessage(content=\"J'aime la programmation.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--83267697-c2fa-4337-8b56-50b3d2f7f8f5-0', usage_metadata={'input_tokens': 14, 'output_tokens': 61, 'total_tokens': 75, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 54}}),\n",
       "  HumanMessage(content='Translate the same thing into chinese', additional_kwargs={}, response_metadata={}, id='422701ba-fb8b-4726-babf-b95a6983e7c2'),\n",
       "  AIMessage(content='我爱编程 (Wǒ ài biānchéng.)', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--4019671a-0a2e-431e-b33e-4d1ef8421ba5-0', usage_metadata={'input_tokens': 29, 'output_tokens': 212, 'total_tokens': 241, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 199}}),\n",
       "  HumanMessage(content='what was my last question?', additional_kwargs={}, response_metadata={}, id='2f7d75cf-f9d9-4de9-b626-ee8b70ee180a'),\n",
       "  AIMessage(content='Your last question was: \"Translate the same thing into chinese\"', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--e239a268-ef79-4128-bc22-3165195b598b-0', usage_metadata={'input_tokens': 50, 'output_tokens': 157, 'total_tokens': 207, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 144}})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\": {\"thread_id\": \"1\"}}\n",
    "app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"what was my last question?\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a dict\n",
    "response = app.invoke({\"messages\": [HumanMessage(content=\"what 2+2?\")]},\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbf11666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#  call the messages key to access the messages\n",
    "print(len(response[\"messages\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71715c53",
   "metadata": {},
   "source": [
    "abit more of an update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b437ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a helpful assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5c340a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x11042e700>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b634bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature = 0.3)\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = system_prompt.invoke(state)\n",
    "    response = gemini.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "memory = InMemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config={\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c3f8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the most important question in life', additional_kwargs={}, response_metadata={}, id='7dbede97-e90f-4eaa-bcd3-af2efb98a59f'),\n",
       "  AIMessage(content='That\\'s a profound question, and the beauty of it is that there isn\\'t one single, universally agreed-upon answer. What constitutes the \"most important question\" often depends on an individual\\'s philosophy, beliefs, life stage, and experiences.\\n\\nHowever, many of the most influential thinkers, philosophers, and spiritual traditions have converged on a few core themes. Here are some of the most common contenders for \"the most important question in life\":\\n\\n1.  **\"What is the meaning of life?\" / \"Why are we here?\"**\\n    *   This is perhaps the quintessential existential question. It seeks to understand our purpose, the ultimate significance of our existence, and the universe itself. Many people spend their entire lives grappling with this.\\n\\n2.  **\"How should I live?\" / \"What is a good life?\"**\\n    *   This question shifts from abstract meaning to practical ethics and well-being. It delves into morality, values, happiness, fulfillment, and how to navigate relationships and responsibilities. It\\'s about the *art* of living.\\n\\n3.  **\"Who am I?\" / \"What is my true self?\"**\\n    *   This is a question of identity and self-discovery. It explores our inner nature, our unique potential, our strengths and weaknesses, and how we relate to the world around us. Understanding oneself is often seen as a prerequisite for understanding anything else.\\n\\n4.  **\"How can I alleviate suffering (my own and others\\')?\"**\\n    *   This question is central to many spiritual and humanitarian traditions. It acknowledges the pervasive nature of suffering in the world and seeks paths to compassion, peace, and liberation.\\n\\n5.  **\"What is truth?\" / \"What is real?\"**\\n    *   This epistemological question seeks to understand the nature of reality, knowledge, and objective truth. It underpins scientific inquiry, philosophical debate, and personal understanding of the world.\\n\\n**Why there\\'s no single answer:**\\n\\n*   **Subjectivity:** What resonates most deeply with one person might not with another.\\n*   **Evolution:** The \"most important question\" might change for an individual throughout their life. A young person might focus on purpose, while an older person might focus on legacy or acceptance.\\n*   **Interconnectedness:** These questions are often deeply intertwined. Finding your purpose (2) might help you understand the meaning of life (1), and knowing yourself (3) is crucial for living well (2).\\n\\nUltimately, the most important question in life might not be a single fixed query, but rather **the ongoing process of asking, seeking, and reflecting.** It\\'s the journey of inquiry itself that often leads to growth, understanding, and a more fulfilling existence.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--5145f31a-447f-411d-9210-0d5669172660-0', usage_metadata={'input_tokens': 14, 'output_tokens': 1606, 'total_tokens': 1620, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1033}})]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define query\n",
    "# wrap humanmessage\n",
    "\n",
    "query= [HumanMessage(\"what is the most important question in life\")]\n",
    "app.invoke(\n",
    "    {\"messages\": query},\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf4b85",
   "metadata": {},
   "source": [
    "now to add more variables\n",
    "you have to update the state of the workflow/application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265028b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "new_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a standup comedian who will tell jokes. and you always tell {topic} number of jokes\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c7146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph, add_messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature = 0.3)\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    topic: str\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "def call_model(state: State):\n",
    "    prompt = new_prompt.invoke(state)\n",
    "    response = gemini.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "memory = InMemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "config={\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3d1c105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Give me a dad joke', additional_kwargs={}, response_metadata={}, id='a9770aec-006c-4bd5-9523-d0b93f9fc64a'),\n",
       "  AIMessage(content=\"Alright, alright, settle down folks! You want a dad joke, eh? I got 'em. Two of 'em, in fact, because that's how I roll.\\n\\nHere's the first one:\\n\\nWhy don't scientists trust atoms?\\nBecause they make up everything!\\n\\nAnd for my second act, prepare yourselves:\\n\\nI told my wife she was drawing her eyebrows too high.\\nShe looked surprised.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--75e4ac5c-3b15-4944-879c-cca695583b17-0', usage_metadata={'input_tokens': 26, 'output_tokens': 139, 'total_tokens': 165, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 50}})],\n",
       " 'topic': '2'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=[HumanMessage(\"Give me a dad joke\")]\n",
    "topic=\"2\"\n",
    "app.invoke(\n",
    "    {\"messages\": input,\n",
    "     \"topic\": topic},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8df0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, alright, settle down folks, you're a great crowd tonight!\n",
      "\n",
      "Here are a couple of zingers for ya:\n",
      "\n",
      "1.  Why did the scarecrow win an award|? Because he was outstanding in his field!\n",
      "2.  What do you call a boomerang that won’t come back? A stick!|"
     ]
    }
   ],
   "source": [
    "input=[HumanMessage(\"Give me a dad joke\")]\n",
    "topic=\"2\"\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input, \"topic\": topic},\n",
    "    config=config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9246aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
