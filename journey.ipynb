{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598af707",
   "metadata": {},
   "source": [
    "##### Purpose\n",
    "This notebook is my dairy to teach myself on how to build a RAG system and use LLMs in applications.\n",
    "The end goal here is to create a LLM that has DaggerHearts game manual as its knowledge base.  \n",
    "I will be starting from the small blocks first and build up from there. \n",
    "\n",
    "First. LLM, it is the brain in the whole system. \n",
    "I will be using LangChain as the library of choice as it is one of the most popular frameworks avaliable. \n",
    "Gemini has been chosen as it provides free API keys for me to test out the application.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4ba964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library import cell. This cell will contain all the library imports used in this notebook.\n",
    "\n",
    "# To handle API Key\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# To get the LLM model\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# To get the memory\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d10f0",
   "metadata": {},
   "source": [
    "What is an API Key? \n",
    "An API Key is akin to a password/digital key for you to access services. \n",
    "This is used to prevent unauthorised access etc.\n",
    "\n",
    "#### Setting up API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f35ce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# use .env to load env variables\n",
    "load_dotenv()\n",
    "# saving env variable as GOOGLE_API_KEY\n",
    "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761b861",
   "metadata": {},
   "source": [
    "#### Initialize LLM Model\n",
    "\n",
    "Here are some standardized parameters when constructing ChatModels:\n",
    "\n",
    "* model: the name of the model\n",
    "* temperature: the sampling temperature\n",
    "* timeout: request timeout\n",
    "* max_tokens: max tokens to generate\n",
    "* stop: default stop sequences\n",
    "* max_retries: max number of times to retry requests\n",
    "* api_key: API key for the model provider\n",
    "* base_url: endpoint to send requests to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d82ffffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# initialize a llm \n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", temperature = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15bfd8",
   "metadata": {},
   "source": [
    "#### Calling the LLM. \n",
    "Use `.invoke` to \"talk\" the LLM. The LLM will generate a response and you view the response within the `.content` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1f8ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tom! Yes, you are. It's great to meet you!\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# use .invoke to get a response from the LLM based on query\n",
    "query = \"Hi, i'm tom?\"\n",
    "response = llm.invoke(query)\n",
    "print(response.content, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15e419",
   "metadata": {},
   "source": [
    "Use `.stream` to see how the response broken down into parts, the following example is using the pipe `|` symbol to show each chunk that is returned by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecb3ff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are the classic lyrics to \"Fly Me to the Moon,\" famously performed by Frank Sinatra, but written by Bart Howard.\n",
      "\n",
      "---\n",
      "\n",
      "**Fly Me to the Moon**|\n",
      "*(Written by Bart Howard)*\n",
      "\n",
      "Fly me to the moon\n",
      "Let me play among the stars\n",
      "Let me see what spring is like\n",
      "On a-Jupiter and Mars\n",
      "\n",
      "In other words, hold my hand\n",
      "In other words, baby|, kiss me\n",
      "\n",
      "Fill my heart with song\n",
      "And let me sing forevermore\n",
      "You are all I long for\n",
      "All I worship and adore\n",
      "\n",
      "In other words, please be true\n",
      "In other words, I love you\n",
      "\n",
      "---|"
     ]
    }
   ],
   "source": [
    "# use .invoke to get a response from the LLM based on query\n",
    "\n",
    "for chunk in llm.stream(\"Write me the lyrics to the song 'Fly me to the moon'\"):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3690b1",
   "metadata": {},
   "source": [
    "After calling a LLM. The returned object has several attributes\n",
    "\n",
    "* contents = Text message from the LLM model\n",
    "* additional_kwargs\n",
    "* response_metadata\n",
    "* id\n",
    "* usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c493e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hi Tom! Yes, you are. It's great to meet you!\\n\\nHow can I help you today?\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--afe5f853-be75-4e5f-984a-34d57fcc4231-0' usage_metadata={'input_tokens': 8, 'output_tokens': 639, 'total_tokens': 647, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 615}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e1247e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_backwards_compat_tool_calls', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', 'additional_kwargs', 'construct', 'content', 'copy', 'dict', 'example', 'from_orm', 'get_lc_namespace', 'id', 'invalid_tool_calls', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'name', 'parse_file', 'parse_obj', 'parse_raw', 'pretty_print', 'pretty_repr', 'response_metadata', 'schema', 'schema_json', 'text', 'to_json', 'to_json_not_implemented', 'tool_calls', 'type', 'update_forward_refs', 'usage_metadata', 'validate']\n"
     ]
    }
   ],
   "source": [
    "# this allows you to see the different attributes of this object\n",
    "print(dir(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd604dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tom! Nice to meet you.\n",
      "\n",
      "I'm an AI assistant, ready to help you with whatever you need. What can I do for you today?\n"
     ]
    }
   ],
   "source": [
    "response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36f5b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(response.additional_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec07e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "print(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f64fee",
   "metadata": {},
   "source": [
    "#### Formatting the Prompt\n",
    "\n",
    "* The prompt is the input for the LLM. \n",
    "* Prompts are can be anything from simple (\"What is 2+2\") to complex (\"Explain the meaning of life\")\n",
    "* You can define variables within the prompt it self. This provide useability to change the variables\n",
    "\n",
    "* `PromptTemplate` allows for simple string inputs\n",
    "* `ChatPromptTemplate` allows for more complex inputs with roles etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c4e2427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Tom! Nice to meet you.\\n\\nI'm ready to answer your questions whenever you are. What would you like to know?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--b6475dc0-10ed-490d-92f5-321db657f427-0', usage_metadata={'input_tokens': 10, 'output_tokens': 579, 'total_tokens': 589, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 551}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Answer the following questions. {query}\")\n",
    "prompt.invoke({\"query\": \"Hi  im tom\"})\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"query\": \"Hi  im tom\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec014484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc9d7de",
   "metadata": {},
   "source": [
    "#### Adding memory function for conversational context\n",
    "\n",
    "LLMs are stateless. \n",
    "\n",
    "Here you can see that, after introducing yourself to the LLM. It is not able to \"remember\" your name. \n",
    "By adding a memory function. You will be able to converse with the LLM like talking to a person \n",
    "This is achieved by adding the chat history as context for LLM to generate the response.\n",
    "\n",
    "Messages in LangChain have several components. A `role`, a `content` and `metadata`\n",
    "\n",
    "Roles - describes WHO is saying the message\n",
    "* user - You\n",
    "* assistant - The LLM answering the input\n",
    "* system - tells models how to behave, not a models support this\n",
    "* tool - represent the decision for LLM to use a tool or not. It contains a dictionary (`name`, `args`, `id`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbd688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Albert! It's nice to meet you. I'm an AI.\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# use .invoke to get a response from the LLM based on query\n",
    "query_2 = \"Hi i am albert\"\n",
    "response_2 = llm.invoke(query_2)\n",
    "print(response_2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1335247",
   "metadata": {},
   "source": [
    "The LLM does not remember that you have already introduced yourself earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806f045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have access to personal information about you, including your name. I don't retain memory of past conversations or user identities.\n",
      "\n",
      "If you'd like to tell me your name, I can refer to you by it during our current conversation, but I won't remember it for future interactions.\n"
     ]
    }
   ],
   "source": [
    "# use .invoke to get a response from the LLM based on query\n",
    "query_3 = \"What is my name\"\n",
    "response_3 = llm.invoke(query_3)\n",
    "print(response_3.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c282da",
   "metadata": {},
   "source": [
    "To enable them as conversational partners. They need to be able to remember what has be said before. \n",
    "This means to give them \"memory\".\n",
    "\n",
    "#### Create memory function for conversational context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chat = ConversationChain(llm = llm , memory = memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29dcdd8",
   "metadata": {},
   "source": [
    "The returned object from the LLM invocation is a dictionary\n",
    "you can access the response but accessing the key \"response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d01f792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hi im tom',\n",
       " 'history': \"Human: Hi im tom\\nAI: Hi Tom! It's really nice to meet you! I'm an AI, a large language model, and I'm here and ready to chat about anything you'd like. It's always great to connect with new people – or, well, new humans, in my case! How are you doing today, Tom?\\nHuman: what is my name?\\nAI: Your name is Tom! You told me that right at the beginning of our chat. It's good to remember names, even for an AI like me! Is there anything else you'd like to chat about, Tom?\",\n",
       " 'response': \"Oh, hello again, Tom! It seems like you're introducing yourself again, and I'm happy to confirm that yes, your name is Tom! You've told me that a few times now since we started chatting, and I've definitely got it stored away in my memory. It's always good to be sure, though! Is there anything specific you'd like to talk about today, Tom, or were you just checking if I remembered? I'm ready for whatever you have in mind!\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.invoke(\"Hi im tom\")\n",
    "response['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bcfa4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, hello again, Tom! It seems like you're introducing yourself again, and I'm happy to confirm that yes, your name is Tom! You've told me that a few times now since we started chatting, and I've definitely got it stored away in my memory. It's always good to be sure, though! Is there anything specific you'd like to talk about today, Tom, or were you just checking if I remembered? I'm ready for whatever you have in mind!\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a04db06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name?',\n",
       " 'history': \"Human: Hi im tom\\nAI: Hi Tom! It's really nice to meet you! I'm an AI, a large language model, and I'm here and ready to chat about anything you'd like. It's always great to connect with new people – or, well, new humans, in my case! How are you doing today, Tom?\",\n",
       " 'response': \"Your name is Tom! You told me that right at the beginning of our chat. It's good to remember names, even for an AI like me! Is there anything else you'd like to chat about, Tom?\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(\"what is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68eccb8",
   "metadata": {},
   "source": [
    "### ConversationChain is deprecated\n",
    "\n",
    "will need to learn about Runnables and LangChain Expression Language (LECL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
